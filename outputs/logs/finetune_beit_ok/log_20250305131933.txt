{ GPU               }-> 0
{ GPU_IDS           }-> [0]
{ CURRENT_GPU       }-> 0
{ N_GPU             }-> 1
{ SEED              }-> 99
{ TIMESTAMP         }-> 20250305131933
{ VERSION           }-> finetune_beit_ok
{ CKPTS_DIR         }-> outputs/ckpts/finetune_beit_ok
{ LOG_PATH          }-> outputs/logs/finetune_beit_ok/log_20250305131933.txt
{ RESULT_DIR        }-> outputs/results/finetune_beit_ok
{ RESULT_PATH       }-> outputs/results/finetune_beit_ok/result_20250305131933.json
{ TASK              }-> ok
{ RUN_MODE          }-> finetune
{ DATA_TAG          }-> ok
{ DATA_MODE         }-> finetune
{ EVAL_NOW          }-> True
{ NUM_WORKERS       }-> 8
{ PIN_MEM           }-> True
{ EXAMPLE_FILE_PATH }-> outputs/results/finetune_beit_ok/examples.json
{ ANSWER_LATENTS_DIR }-> outputs/results/finetune_beit_ok/answer_latents
{ n_shots           }-> 10
{ k_ensemble        }-> 5
{ no_of_captions    }-> 9
{ multiple_choice   }-> False
{ finetune          }-> True
{ gen_examples      }-> False
{ cfg_file          }-> configs/finetune.yml
{ PRETRAINED_MODEL_PATH }-> /Visual-Question-Answering/beit_pretrained/mp_rank_00_model_states.pt
{ RESUME            }-> False
{ GRAD_ACCU_STEPS   }-> 2
{ MAX_TOKEN         }-> 32
{ BATCH_SIZE        }-> 8
{ EVAL_BATCH_SIZE   }-> 8
{ BERT_LR_MULT      }-> 0.01
{ LR_BASE           }-> 5e-05
{ LR_DECAY_R        }-> 0.2
{ LR_DECAY_LIST     }-> [5]
{ WARMUP_EPOCH      }-> 0
{ MAX_EPOCH         }-> 8
{ GRAD_NORM_CLIP    }-> -1
{ OPT               }-> AdamW
{ OPT_PARAMS        }-> {'betas': '(0.9, 0.98)', 'eps': '1e-9'}
{ EPOPH_FTW         }-> 1
{ OPT_FTW           }-> Adam
{ LR_BASE_FTW       }-> 0.001
{ OPT_PARAMS_FTW    }-> {'betas': '(0.9, 0.98)', 'eps': '1e-9'}

